{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coursera Depp learning specialization 첫 강의 리뷰겸 만들어보는 간단한 뉴럴넷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>밑바닥부터 짜는 로지스틱 회귀</h1>\n",
    "<h2>사용되는 수식들...</h2>\n",
    "<ul>\n",
    "    <li>Logistic regression : $\\sigma(Z) = \\sigma(W A + b)$</li>\n",
    "    <li>Cost function : $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$</li>\n",
    "    <li>\n",
    "        <h2>Backward propagation</h2>\n",
    "        <ul>\n",
    "            <li>$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$</li>\n",
    "            <li>$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$</li>\n",
    "            <li>$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Activation function</h2>\n",
    "<ul>\n",
    "    <li>Hidden layer : Relu</li>\n",
    "    <li>Output layer : Sigmoid</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h3><a href=\"https://www.coursera.org/specializations/deep-learning?\">Coursera 강의 실습 참조</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def relu(Z):\n",
    "    cache = Z\n",
    "    A = tf.nn.relu(Z)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    cache = Z\n",
    "    A = tf.math.sigmoid(Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init parameters\n",
    "def initialize_parameters(layer_dims, alpha=0.01):\n",
    "    # parameter layer_dims : Array of dimesions of layers.\n",
    "    np.random.seed(1)\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    print(L)\n",
    "    \n",
    "    # store paremters for each layer.\n",
    "    parameters = {}\n",
    "    \n",
    "    # first layer is input layer.\n",
    "    # last layer is output layer.\n",
    "    for l in range (1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * alpha\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward(A, W, b):    \n",
    "    # Calculate Z value.\n",
    "    Z = tf.tensordot(W.T, A, axes=1) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "    \n",
    "    \n",
    "def forward_propagation(A_prev, W, b, activation=\"sigmoid\"):\n",
    "    Z, param_cache = forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, Z_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, Z_cache = relu(Z)\n",
    "        \n",
    "    cache = (param_cache, Z_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def layered_forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range (1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_propagation(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        \n",
    "    AL, cache = forward_propagation(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969,  0.00865408,\n",
      "        -0.02301539,  0.01744812, -0.00761207,  0.00319039, -0.0024937 ],\n",
      "       [ 0.01462108, -0.02060141, -0.00322417, -0.00384054,  0.01133769,\n",
      "        -0.01099891, -0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
      "       [-0.01100619,  0.01144724,  0.00901591,  0.00502494,  0.00900856,\n",
      "        -0.00683728, -0.0012289 , -0.00935769, -0.00267888,  0.00530355],\n",
      "       [-0.00691661, -0.00396754, -0.00687173, -0.00845206, -0.00671246,\n",
      "        -0.00012665, -0.0111731 ,  0.00234416,  0.01659802,  0.00742044],\n",
      "       [-0.00191836, -0.00887629, -0.00747158,  0.01692455,  0.00050808,\n",
      "        -0.00636996,  0.00190915,  0.02100255,  0.00120159,  0.00617203],\n",
      "       [ 0.0030017 , -0.0035225 , -0.01142518, -0.00349343, -0.00208894,\n",
      "         0.00586623,  0.00838983,  0.00931102,  0.00285587,  0.00885141],\n",
      "       [-0.00754398,  0.01252868,  0.0051293 , -0.00298093,  0.00488518,\n",
      "        -0.00075572,  0.01131629,  0.01519817,  0.02185575, -0.01396496],\n",
      "       [-0.01444114, -0.00504466,  0.00160037,  0.00876169,  0.00315635,\n",
      "        -0.02022201, -0.00306204,  0.00827975,  0.00230095,  0.00762011],\n",
      "       [-0.00222328, -0.00200758,  0.00186561,  0.00410052,  0.001983  ,\n",
      "         0.00119009, -0.00670662,  0.00377564,  0.00121821,  0.01129484],\n",
      "       [ 0.01198918,  0.00185156, -0.00375285, -0.0063873 ,  0.00423494,\n",
      "         0.0007734 , -0.00343854,  0.00043597, -0.00620001,  0.00698032]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.00447129,  0.01224508,  0.00403492,  0.00593579, -0.01094912,\n",
      "         0.00169382,  0.00740556, -0.00953701, -0.00266219,  0.00032615],\n",
      "       [-0.01373117,  0.00315159,  0.00846161, -0.00859516,  0.00350546,\n",
      "        -0.01312283, -0.00038696, -0.01615772,  0.01121418,  0.00408901],\n",
      "       [-0.00024617, -0.00775162,  0.01273756,  0.01967102, -0.01857982,\n",
      "         0.01236164,  0.01627651,  0.00338012, -0.01199268,  0.00863345],\n",
      "       [-0.0018092 , -0.00603921, -0.01230058,  0.00550537,  0.00792807,\n",
      "        -0.00623531,  0.00520576, -0.01144341,  0.00801861,  0.00046567],\n",
      "       [-0.0018657 , -0.00101746,  0.00868886,  0.00750412,  0.00529465,\n",
      "         0.00137701,  0.00077821,  0.0061838 ,  0.00232495,  0.00682551],\n",
      "       [-0.00310117, -0.02434838,  0.01038825,  0.0218698 ,  0.00441364,\n",
      "        -0.00100155, -0.00136445, -0.00119054,  0.00017409, -0.01122019],\n",
      "       [-0.00517094, -0.00997027,  0.00248799, -0.00296641,  0.00495211,\n",
      "        -0.00174703,  0.00986335,  0.00213534,  0.021907  , -0.01896361],\n",
      "       [-0.00646917,  0.00901487,  0.02528326, -0.00248635,  0.00043669,\n",
      "        -0.00226314,  0.01331457, -0.00287308,  0.0068007 , -0.00319802],\n",
      "       [-0.01272559,  0.00313548,  0.00503185,  0.01293226, -0.00110447,\n",
      "        -0.00617362,  0.00562761,  0.00240737,  0.00280665, -0.00073113],\n",
      "       [ 0.01160339,  0.00369493,  0.01904659,  0.01111057,  0.0065905 ,\n",
      "        -0.01627438,  0.00602319,  0.00420282,  0.00810952,  0.01044442]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}\n",
      "4\n",
      "L is :  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       " array([[0.49996254],\n",
       "        [0.50002304],\n",
       "        [0.50015402],\n",
       "        [0.50021609],\n",
       "        [0.49991345],\n",
       "        [0.4999705 ],\n",
       "        [0.50009549],\n",
       "        [0.49989358],\n",
       "        [0.49999173],\n",
       "        [0.49997211]])>,\n",
       " [((array([[-0.40087819],\n",
       "           [ 0.82400562],\n",
       "           [-0.56230543],\n",
       "           [ 1.95487808],\n",
       "           [-1.33195167],\n",
       "           [-1.76068856],\n",
       "           [-1.65072127],\n",
       "           [-0.89055558],\n",
       "           [-1.1191154 ],\n",
       "           [ 1.9560789 ]]),\n",
       "    array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969,  0.00865408,\n",
       "            -0.02301539,  0.01744812, -0.00761207,  0.00319039, -0.0024937 ],\n",
       "           [ 0.01462108, -0.02060141, -0.00322417, -0.00384054,  0.01133769,\n",
       "            -0.01099891, -0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
       "           [-0.01100619,  0.01144724,  0.00901591,  0.00502494,  0.00900856,\n",
       "            -0.00683728, -0.0012289 , -0.00935769, -0.00267888,  0.00530355],\n",
       "           [-0.00691661, -0.00396754, -0.00687173, -0.00845206, -0.00671246,\n",
       "            -0.00012665, -0.0111731 ,  0.00234416,  0.01659802,  0.00742044],\n",
       "           [-0.00191836, -0.00887629, -0.00747158,  0.01692455,  0.00050808,\n",
       "            -0.00636996,  0.00190915,  0.02100255,  0.00120159,  0.00617203],\n",
       "           [ 0.0030017 , -0.0035225 , -0.01142518, -0.00349343, -0.00208894,\n",
       "             0.00586623,  0.00838983,  0.00931102,  0.00285587,  0.00885141],\n",
       "           [-0.00754398,  0.01252868,  0.0051293 , -0.00298093,  0.00488518,\n",
       "            -0.00075572,  0.01131629,  0.01519817,  0.02185575, -0.01396496],\n",
       "           [-0.01444114, -0.00504466,  0.00160037,  0.00876169,  0.00315635,\n",
       "            -0.02022201, -0.00306204,  0.00827975,  0.00230095,  0.00762011],\n",
       "           [-0.00222328, -0.00200758,  0.00186561,  0.00410052,  0.001983  ,\n",
       "             0.00119009, -0.00670662,  0.00377564,  0.00121821,  0.01129484],\n",
       "           [ 0.01198918,  0.00185156, -0.00375285, -0.0063873 ,  0.00423494,\n",
       "             0.0007734 , -0.00343854,  0.00043597, -0.00620001,  0.00698032]]),\n",
       "    array([[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]])),\n",
       "   <tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       "   array([[ 0.04672755],\n",
       "          [-0.02101165],\n",
       "          [-0.00829549],\n",
       "          [-0.05456858],\n",
       "          [-0.01412357],\n",
       "          [ 0.02135342],\n",
       "          [-0.06205491],\n",
       "          [-0.07453996],\n",
       "          [-0.02522432],\n",
       "          [ 0.01080047]])>),\n",
       "  ((<tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       "    array([[0.04672755],\n",
       "           [0.        ],\n",
       "           [0.        ],\n",
       "           [0.        ],\n",
       "           [0.        ],\n",
       "           [0.02135342],\n",
       "           [0.        ],\n",
       "           [0.        ],\n",
       "           [0.        ],\n",
       "           [0.01080047]])>,\n",
       "    array([[-0.00447129,  0.01224508,  0.00403492,  0.00593579, -0.01094912,\n",
       "             0.00169382,  0.00740556, -0.00953701, -0.00266219,  0.00032615],\n",
       "           [-0.01373117,  0.00315159,  0.00846161, -0.00859516,  0.00350546,\n",
       "            -0.01312283, -0.00038696, -0.01615772,  0.01121418,  0.00408901],\n",
       "           [-0.00024617, -0.00775162,  0.01273756,  0.01967102, -0.01857982,\n",
       "             0.01236164,  0.01627651,  0.00338012, -0.01199268,  0.00863345],\n",
       "           [-0.0018092 , -0.00603921, -0.01230058,  0.00550537,  0.00792807,\n",
       "            -0.00623531,  0.00520576, -0.01144341,  0.00801861,  0.00046567],\n",
       "           [-0.0018657 , -0.00101746,  0.00868886,  0.00750412,  0.00529465,\n",
       "             0.00137701,  0.00077821,  0.0061838 ,  0.00232495,  0.00682551],\n",
       "           [-0.00310117, -0.02434838,  0.01038825,  0.0218698 ,  0.00441364,\n",
       "            -0.00100155, -0.00136445, -0.00119054,  0.00017409, -0.01122019],\n",
       "           [-0.00517094, -0.00997027,  0.00248799, -0.00296641,  0.00495211,\n",
       "            -0.00174703,  0.00986335,  0.00213534,  0.021907  , -0.01896361],\n",
       "           [-0.00646917,  0.00901487,  0.02528326, -0.00248635,  0.00043669,\n",
       "            -0.00226314,  0.01331457, -0.00287308,  0.0068007 , -0.00319802],\n",
       "           [-0.01272559,  0.00313548,  0.00503185,  0.01293226, -0.00110447,\n",
       "            -0.00617362,  0.00562761,  0.00240737,  0.00280665, -0.00073113],\n",
       "           [ 0.01160339,  0.00369493,  0.01904659,  0.01111057,  0.0065905 ,\n",
       "            -0.01627438,  0.00602319,  0.00420282,  0.00810952,  0.01044442]]),\n",
       "    array([[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]])),\n",
       "   <tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       "   array([[-1.49830682e-04],\n",
       "          [ 9.21681888e-05],\n",
       "          [ 6.16078514e-04],\n",
       "          [ 8.64359060e-04],\n",
       "          [-3.46198524e-04],\n",
       "          [-1.18009372e-04],\n",
       "          [ 3.81961574e-04],\n",
       "          [-4.25670561e-04],\n",
       "          [-3.30932453e-05],\n",
       "          [-1.11544717e-04]])>)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(10, 1)\n",
    "params = initialize_parameters([10, 10, 10])\n",
    "layered_forward_propagation(X, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_relu(dA, cache):\n",
    "    # parameter cache == Z_cache\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA)\n",
    "    dZ[Z<=0] = 0\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def backward_sigmoid(dA, cache):\n",
    "    # parameter cache == Z_cache\n",
    "    Z = cache\n",
    "    \n",
    "    y = tf.nn.sigmoid(Z)\n",
    "    \n",
    "    dZ = dA / (y * (1-y))\n",
    "    \n",
    "    return dZ\n",
    "    \n",
    "    \n",
    "# Compute cost\n",
    "def cost(AL, Y):\n",
    "    # m == Number of train sets\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(AL) + (1 - Y) * tf.math.log(1 - AL))\n",
    "    \n",
    "    cost = tf.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "def backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = tf.tensordot(dZ, tf.transpose(A_prev), axes=1) / m\n",
    "    db = tf.reduce_sum(dZ) / m\n",
    "    dA_prev = tf.tensordot(W, dZ, axes=1)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def backward_propagation(dA, cache, activation=\"sigmoid\"):\n",
    "    param_cache, Z_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = backward_relu(dA, Z_cache)\n",
    "        print(\"relu dZ: \", dZ)\n",
    " \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = backward_sigmoid(dA, Z_cache)\n",
    "        print(\"sigmoid dZ: \", dZ)\n",
    "    \n",
    "    dA_prev, dW, db = backward(dZ, param_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def layered_backward_propagation(AL, Y, caches):\n",
    "    grads = {}    \n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(tf.math.divide(Y, AL) - tf.math.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_propagation(dAL, current_cache)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = backward_propagation(grads[\"dA\" + str(l + 1)], current_cache)\n",
    "        \n",
    "    return grads\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "L is :  2\n",
      "sigmoid dZ:  tf.Tensor(\n",
      "[[-8.00059941]\n",
      " [-7.99963136]\n",
      " [-7.9975372 ]\n",
      " [-7.99654555]\n",
      " [-8.00138527]\n",
      " [-8.00047209]\n",
      " [-7.99847274]\n",
      " [-8.00170341]\n",
      " [-8.00013238]\n",
      " [-8.00044623]], shape=(10, 1), dtype=float64)\n",
      "sigmoid dZ:  tf.Tensor(\n",
      "[[-0.12844599]\n",
      " [ 0.6904823 ]\n",
      " [-1.1031659 ]\n",
      " [ 0.34281303]\n",
      " [-1.1549548 ]\n",
      " [ 0.17256866]\n",
      " [-0.08093373]\n",
      " [-1.20325212]\n",
      " [-0.35836319]\n",
      " [-2.0654111 ]], shape=(10, 1), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dA1': <tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       " array([[-0.03209398],\n",
       "        [ 0.17260152],\n",
       "        [-0.27578673],\n",
       "        [ 0.08563949],\n",
       "        [-0.2887243 ],\n",
       "        [ 0.04313725],\n",
       "        [-0.02021397],\n",
       "        [-0.30039557],\n",
       "        [-0.08957655],\n",
       "        [-0.51633772]])>,\n",
       " 'dW2': <tf.Tensor: shape=(10, 10), dtype=float64, numpy=\n",
       " array([[-0.37384838,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17084017,  0.        ,  0.        ,  0.        , -0.08641027],\n",
       "        [-0.37380314,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.1708195 ,  0.        ,  0.        ,  0.        , -0.08639981],\n",
       "        [-0.37370529,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17077479,  0.        ,  0.        ,  0.        , -0.08637719],\n",
       "        [-0.37365895,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17075361,  0.        ,  0.        ,  0.        , -0.08636648],\n",
       "        [-0.3738851 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17085696,  0.        ,  0.        ,  0.        , -0.08641875],\n",
       "        [-0.37384243,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17083746,  0.        ,  0.        ,  0.        , -0.08640889],\n",
       "        [-0.373749  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17079476,  0.        ,  0.        ,  0.        , -0.0863873 ],\n",
       "        [-0.37389997,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.17086375,  0.        ,  0.        ,  0.        , -0.08642219],\n",
       "        [-0.37382656,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.1708302 ,  0.        ,  0.        ,  0.        , -0.08640522],\n",
       "        [-0.37384122,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.1708369 ,  0.        ,  0.        ,  0.        , -0.08640861]])>,\n",
       " 'db2': <tf.Tensor: shape=(), dtype=float64, numpy=-79.99692564579198>,\n",
       " 'dA0': <tf.Tensor: shape=(10, 1), dtype=float64, numpy=\n",
       " array([[-0.00637464],\n",
       "        [-0.03034172],\n",
       "        [-0.00912487],\n",
       "        [-0.0126279 ],\n",
       "        [-0.03212849],\n",
       "        [-0.01917431],\n",
       "        [-0.00102512],\n",
       "        [-0.03380336],\n",
       "        [-0.0316032 ],\n",
       "        [-0.01551049]])>,\n",
       " 'dW1': <tf.Tensor: shape=(10, 10), dtype=float64, numpy=\n",
       " array([[ 0.0514912 , -0.10584022,  0.07222588, -0.25109625,  0.17108385,\n",
       "          0.22615339,  0.21202853,  0.1143883 ,  0.14374589, -0.2512505 ],\n",
       "        [-0.2767993 ,  0.5689613 , -0.38826195,  1.34980871, -0.91968905,\n",
       "         -1.21572429, -1.13979382, -0.61491287, -0.77272938,  1.35063786],\n",
       "        [ 0.44223515, -0.9090149 ,  0.62031618, -2.15655483,  1.46936366,\n",
       "          1.94233158,  1.82101941,  0.98243055,  1.23456994, -2.15787954],\n",
       "        [-0.13742627,  0.28247987, -0.19276563,  0.67015769, -0.45661039,\n",
       "         -0.60358699, -0.56588877, -0.30529406, -0.38364735,  0.67056934],\n",
       "        [ 0.46299619, -0.95168925,  0.64943736, -2.25779582,  1.53834397,\n",
       "          2.03351571,  1.90650845,  1.02855145,  1.2925277 , -2.25918272],\n",
       "        [-0.06917901,  0.14219755, -0.0970363 ,  0.3373507 , -0.22985312,\n",
       "         -0.30383967, -0.28486276, -0.15368199, -0.19312425,  0.33755792],\n",
       "        [ 0.03244457, -0.06668984,  0.04550947, -0.15821556,  0.10779981,\n",
       "          0.14249908,  0.13359902,  0.07207598,  0.09057418, -0.15831275],\n",
       "        [ 0.48235753, -0.99148651,  0.6765952 , -2.35221118,  1.60267366,\n",
       "          2.11855224,  1.98623386,  1.07156289,  1.34657797, -2.35365608],\n",
       "        [ 0.14365999, -0.29529328,  0.20150957, -0.70055634,  0.47732244,\n",
       "          0.63096596,  0.59155773,  0.31914234,  0.40104976, -0.70098667],\n",
       "        [ 0.82797827, -1.70191035,  1.16139188, -4.03762688,  2.75102776,\n",
       "          3.6365457 ,  3.40941803,  1.83936339,  2.31143337, -4.04010708]])>,\n",
       " 'db1': <tf.Tensor: shape=(), dtype=float64, numpy=-4.88866282266919>}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(10, 1)\n",
    "params = initialize_parameters([10, 10, 10])\n",
    "AL, caches = layered_forward_propagation(X, params)\n",
    "#Y = np.array([[1.],\n",
    "#              [1.], \n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.],\n",
    "#              [1.]])\n",
    "\n",
    "Y = np.array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "layered_backward_propagation(AL, Y, caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters.\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    L = len(paramters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
